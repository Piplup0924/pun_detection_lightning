{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianmo.cxh/software/miniconda3/envs/pun_detection/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"./bert-base-chinese/\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"你好\"\n",
    "input_ids = torch.tensor([tokenizer.encode(input_text, add_special_tokens=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 5245, 7807,  102]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  872, 1962,  102]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ C L S ]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"你好\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  872, 1962,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.2852,  0.4501,  0.2862,  ...,  0.4362,  0.4979, -0.2372],\n",
       "         [-0.2079,  0.0369,  0.0398,  ..., -0.5650,  0.3733,  0.0951],\n",
       "         [ 0.7796,  0.0815, -0.0798,  ..., -0.0146,  0.8973, -0.2835],\n",
       "         [-0.3637,  0.3618,  0.1234,  ...,  0.6489,  0.6392, -0.2132]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9939,  0.9999,  0.9605,  0.9619,  0.8260,  0.8557, -0.4677, -0.5561,\n",
       "          0.9954, -0.9910,  1.0000,  0.9811, -0.7397, -0.9134,  0.9996, -0.9983,\n",
       "         -0.8414,  0.9996,  0.9935,  0.0913,  0.9988, -1.0000, -0.9656,  0.3223,\n",
       "          0.0429,  0.9642,  0.7766, -0.2238, -0.9999,  0.9853,  0.7119,  0.9974,\n",
       "          0.8192, -1.0000, -0.9977,  0.2347,  0.0275,  0.9781, -0.7894, -0.9872,\n",
       "         -0.9213, -0.7144,  0.5856, -0.9844, -0.9948,  0.3433, -1.0000, -0.9999,\n",
       "         -0.0335,  0.9995, -0.9089, -0.9999,  0.1662, -0.1775, -0.4278,  0.9861,\n",
       "         -0.9997,  0.8741,  1.0000,  0.9624,  0.9993, -0.9792,  0.6728, -0.9992,\n",
       "          1.0000, -0.9998, -0.9687,  0.5746,  0.9998,  1.0000, -0.6966,  0.9984,\n",
       "          1.0000,  0.8680,  0.4717,  0.9553, -0.9917,  0.6155, -1.0000,  0.9033,\n",
       "          1.0000,  0.9976, -0.9693,  0.9825, -0.9781, -0.9998, -0.9998,  0.9984,\n",
       "         -0.4795,  0.9979,  0.9941, -0.9973, -1.0000,  0.9891, -0.9965, -0.9992,\n",
       "         -0.8243,  0.9906, -0.7957,  0.0788, -0.9653,  0.9398, -0.9531, -0.9455,\n",
       "          0.9907,  0.9959,  0.3088, -0.9982,  0.9996,  0.9095, -1.0000, -0.9707,\n",
       "         -0.9855, -0.6870, -0.9751,  0.9998,  0.1588, -0.5762,  0.9976, -0.9208,\n",
       "          0.9357, -0.9998, -0.9667, -0.7951,  0.9937,  0.9998,  0.9826, -0.9992,\n",
       "          0.9764,  1.0000,  0.7163,  0.9705, -0.9500,  0.9953,  0.8976, -0.9930,\n",
       "          0.2421, -0.7597,  1.0000,  0.9947,  0.8938, -0.9663,  0.9993, -0.9960,\n",
       "          0.9997, -1.0000,  0.9982, -1.0000, -0.9995,  0.9938,  0.9480,  1.0000,\n",
       "         -0.9838,  1.0000, -0.9899, -0.9996,  0.9998,  0.6151,  0.9988, -0.9998,\n",
       "          0.9726, -0.7072, -0.2573, -0.3102, -1.0000,  0.9998, -0.8503,  0.9999,\n",
       "          0.9962, -0.9980, -0.9559, -0.9917,  0.7594, -0.9877, -0.9881,  0.9728,\n",
       "          0.2399,  0.9953, -0.5571, -0.9263,  0.9510, -0.8625, -0.9999,  0.9867,\n",
       "         -0.4874,  0.9500, -0.0654,  0.5004,  0.9899,  0.9955, -0.4803,  0.9999,\n",
       "          0.2714,  0.9844,  0.9835, -0.1128, -0.9281, -0.9743, -0.9999,  0.1768,\n",
       "          1.0000, -0.4480, -0.9936,  0.8336, -0.9999,  0.8634,  0.0105, -0.0850,\n",
       "         -0.9952, -0.9997,  0.9996, -0.9573, -0.9828,  0.7724, -0.9732, -0.2547,\n",
       "         -0.9998,  0.8021,  0.9884,  0.6924,  0.6030, -0.3711, -0.9985,  0.9961,\n",
       "         -0.9963,  0.1137,  0.9925,  1.0000,  0.8529, -0.5981, -0.1157,  0.9843,\n",
       "          0.7385, -1.0000,  0.9513, -0.9962, -0.9433,  0.9997, -0.9965,  0.9812,\n",
       "          0.9999,  0.5326,  1.0000,  0.4093, -0.9995, -0.9899,  1.0000,  0.9871,\n",
       "          0.9997, -0.9788, -0.9984,  0.3591, -0.7882, -1.0000, -0.9996, -0.7171,\n",
       "          0.9944,  1.0000,  0.6712, -0.9998, -0.9646, -0.9929,  1.0000, -0.9672,\n",
       "          1.0000,  0.9789, -0.9508, -0.9563,  0.7452, -0.9142, -0.9984, -0.3078,\n",
       "         -0.9997, -0.9873, -0.9998,  0.9569, -0.9958, -1.0000,  0.9706,  0.9998,\n",
       "          0.4959, -1.0000,  0.9918,  0.9981, -0.9888, -0.9988,  0.7732, -1.0000,\n",
       "          1.0000, -0.9885,  0.5371, -0.6624, -0.9802, -0.9412,  0.9995,  0.9895,\n",
       "         -0.8490, -0.9426, -0.9566, -0.9992, -0.4545,  0.9836, -0.9678,  0.9544,\n",
       "         -0.7686, -0.9949,  0.9892, -0.9978, -0.9998,  0.4318,  1.0000, -0.2723,\n",
       "          1.0000,  0.9546,  1.0000,  0.3605, -0.9834,  0.9961,  0.0038, -0.8954,\n",
       "         -0.9380, -0.9866,  0.6333, -0.1696, -0.1036, -0.9996,  1.0000,  0.9973,\n",
       "          0.9246,  0.6614, -0.7647,  0.3073,  0.9638, -0.9863,  0.9959, -0.9975,\n",
       "         -0.9305,  0.9988,  0.9999,  0.9994,  0.7698, -0.6722,  0.9988, -0.9782,\n",
       "          0.9981, -0.9982,  0.9965, -0.9986,  0.4097, -0.6040, -0.9810,  1.0000,\n",
       "          0.9155,  0.1862,  0.9998, -0.9822,  0.9979,  0.9886,  0.9920,  0.8669,\n",
       "          0.6766,  0.9997, -0.8949, -0.9459, -0.7967, -0.9839, -0.9947, -1.0000,\n",
       "          0.6394, -0.9861, -0.9533, -0.6723,  0.7538,  0.8880, -0.9378, -0.1972,\n",
       "         -0.5688,  0.8525, -0.0176,  0.2017,  0.9777, -0.9945, -0.8737, -1.0000,\n",
       "         -0.9977,  0.9360,  0.9999, -0.9999,  0.9592, -0.9999, -0.9961,  0.9714,\n",
       "         -0.7220, -0.8338,  0.9984, -1.0000, -0.0486,  0.9995,  1.0000,  0.9962,\n",
       "          0.9999, -0.0041, -0.9995, -0.9996, -1.0000, -1.0000, -0.9986,  0.8001,\n",
       "          0.9348, -0.9999, -0.8427,  0.8976,  1.0000,  0.9779, -0.9985, -0.6971,\n",
       "         -0.9916, -0.9993,  0.9649, -0.9296, -0.9993,  0.9994, -0.6135,  0.9998,\n",
       "         -0.8385,  0.7521,  0.9796,  0.9864,  0.9161, -0.9999,  0.9484,  1.0000,\n",
       "          0.9628, -0.9999, -0.5728, -0.9078, -0.9999, -0.4131,  0.7792,  0.9994,\n",
       "         -1.0000, -0.4215, -0.9866,  0.5357,  0.9522,  0.9993,  0.9987,  0.9705,\n",
       "          0.9178,  0.9066,  0.4034,  0.9999,  0.2559, -0.9972,  0.9852,  0.6622,\n",
       "          0.8172, -0.9806,  0.9954,  0.8712,  1.0000,  0.9906, -0.0962, -0.9994,\n",
       "         -0.9752,  0.9873,  1.0000, -0.8441, -0.3700, -0.9996, -0.9990, -0.9958,\n",
       "         -0.8604,  0.3345, -0.9633, -0.9980,  0.0698,  0.7781,  1.0000,  1.0000,\n",
       "          0.9993, -0.9883, -0.8637,  0.9731,  0.5644,  0.9925, -0.8145, -1.0000,\n",
       "         -0.9985, -0.9988,  0.9997,  0.3839, -0.8146, -0.5326, -0.1848,  0.8199,\n",
       "         -0.9994, -0.9501, -0.9939, -0.2651,  1.0000, -0.9970,  0.9959, -0.9893,\n",
       "          0.7318,  0.4283,  0.8997,  0.9997, -0.9129, -0.2959, -0.8728,  0.9794,\n",
       "          0.9812,  0.9816, -0.9935,  0.9293,  0.9914, -0.9977,  0.9993,  0.7155,\n",
       "          0.9334,  0.9475,  0.9999,  0.3486,  0.9991,  0.9750,  0.9996,  0.9999,\n",
       "         -0.9971,  0.8065,  0.7472, -0.9761, -0.9044,  0.9896,  0.9998, -0.2708,\n",
       "         -0.9424, -0.9997,  0.9955,  0.9994,  1.0000, -0.8281,  0.9982, -0.9688,\n",
       "          0.9904,  0.8456, -0.1286, -0.5463,  0.7607,  0.9952,  0.9921, -0.9991,\n",
       "         -0.9999, -1.0000,  1.0000,  0.9997, -0.4729, -1.0000,  0.9940, -0.7109,\n",
       "          0.9767,  0.9838,  0.6759, -0.9690,  0.9827, -0.9987,  0.5680,  0.8159,\n",
       "         -0.1421,  0.6964,  0.9957, -0.9991,  0.4680,  1.0000, -0.6458,  1.0000,\n",
       "          0.6797, -0.9994,  0.9995, -0.9976, -0.9990,  0.1149,  0.9999,  0.9934,\n",
       "          0.6907, -0.4217,  0.9995, -0.9997,  0.9998, -0.9999, -0.4501, -0.9977,\n",
       "          0.9997, -0.9962, -0.9747, -0.8210,  0.5362,  0.9977, -0.8071,  0.9997,\n",
       "         -0.5565, -0.9703, -0.9127, -0.9892, -0.9983, -0.9852, -0.0539, -1.0000,\n",
       "          0.4597,  0.0879, -0.8476, -0.8455, -1.0000,  0.9998, -0.9467, -0.9841,\n",
       "          0.9998, -0.9992, -1.0000,  0.9720, -0.9290,  0.6825,  0.9889,  0.8994,\n",
       "         -0.1129, -1.0000,  0.7118,  0.9999, -0.9341, -0.9752, -0.9547, -0.9994,\n",
       "          0.9511,  0.9880,  0.9404,  0.4543,  0.9112,  0.8575,  0.7694, -0.1020,\n",
       "          0.8345, -0.9979, -0.9981, -0.9937, -0.9278, -0.9999, -1.0000,  1.0000,\n",
       "          0.9995,  1.0000,  0.7141, -0.8386,  0.7418,  0.9977, -0.9992, -0.9224,\n",
       "          0.9472,  0.5346, -0.0099, -0.9954, -0.8015, -0.9999, -0.6457,  0.1877,\n",
       "         -0.9927, -0.3894,  0.9999,  0.9997, -0.9982, -0.9760, -0.9537, -0.9985,\n",
       "          0.9996,  0.9900,  0.9963, -0.9528, -0.5002,  0.9103, -0.9392, -0.4958,\n",
       "         -0.9924, -0.9599, -0.9999,  0.8507, -0.9960, -0.9999,  0.9946,  1.0000,\n",
       "          0.9438, -0.9999, -0.0338,  0.9999,  0.9644,  1.0000,  0.9073,  0.9996,\n",
       "         -0.9892,  0.9967, -0.9995,  1.0000, -1.0000,  0.9999,  1.0000,  0.9769,\n",
       "          0.9949, -0.9885,  0.9903, -0.4366, -0.0527,  0.6180, -0.8440, -0.9910,\n",
       "          0.6916,  0.8773, -0.9623,  1.0000,  0.5400,  0.1334,  0.9674,  0.0352,\n",
       "          0.9852,  0.6838, -0.9695,  0.9957,  0.9998,  0.9922,  1.0000,  0.8855,\n",
       "          1.0000, -0.8583, -0.9983,  0.9737, -0.3897, -0.2048, -0.9998,  0.9999,\n",
       "          0.9776, -0.9998, -0.8955, -0.3117,  0.7913,  1.0000,  0.9884,  0.9826,\n",
       "          0.9551,  0.3536,  0.9968, -0.9371,  0.9408, -0.9938, -0.4796,  1.0000,\n",
       "         -0.9733,  0.9979, -0.9611,  0.9839, -0.9882,  0.7802,  0.9323,  0.9373,\n",
       "         -0.9688,  1.0000,  0.8138, -0.9910, -0.9985, -0.9959, -0.9615,  0.6718]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=(tensor([[[ 0.0588,  0.0704, -0.2139,  ..., -0.0237, -0.2234, -0.1116],\n",
       "         [-0.1072, -0.2564,  0.8840,  ..., -0.7744,  0.4134,  0.4990],\n",
       "         [-0.7104, -0.8296, -1.1179,  ..., -0.7519,  0.4980, -0.4506],\n",
       "         [ 0.1253, -0.1008,  0.2129,  ..., -0.4602, -0.1616,  0.7839]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 8.4279e-02, -3.5380e-02, -2.9467e-01,  ...,  1.9545e-02,\n",
       "          -4.4945e-02,  1.1248e-03],\n",
       "         [ 6.6118e-01, -1.4172e-01,  3.2850e-01,  ..., -1.2894e+00,\n",
       "           7.8252e-01,  2.9744e-01],\n",
       "         [-4.0315e-01, -9.2941e-01, -1.9529e+00,  ..., -1.0278e+00,\n",
       "           6.2155e-01, -2.1005e-01],\n",
       "         [-7.4209e-02, -5.8299e-02, -7.4619e-03,  ..., -6.2182e-01,\n",
       "          -8.8080e-03,  1.0506e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-3.1264e-04,  3.8962e-02, -1.2953e-01,  ..., -2.6758e-02,\n",
       "          -1.7605e-02,  6.6592e-02],\n",
       "         [ 5.7143e-01, -4.3601e-01,  3.4067e-01,  ..., -5.0987e-01,\n",
       "           5.9391e-01,  3.3477e-01],\n",
       "         [-4.6542e-02, -1.0470e+00, -1.9754e+00,  ..., -1.1899e+00,\n",
       "           5.0400e-01, -3.5623e-01],\n",
       "         [ 1.0935e-01, -8.8653e-02,  2.2883e-01,  ..., -2.7204e-01,\n",
       "          -1.8235e-01,  1.2776e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0336,  0.1457, -0.0258,  ...,  0.2611, -0.2580,  0.1307],\n",
       "         [ 1.0326, -0.1464,  0.2420,  ...,  0.5979,  0.8785,  0.5494],\n",
       "         [ 0.4249, -0.4439, -1.4372,  ..., -0.6597,  0.5412,  0.7247],\n",
       "         [ 0.0111, -0.0237,  0.0334,  ..., -0.0570, -0.0893, -0.0209]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0512,  0.2764, -0.0277,  ...,  0.4218, -0.1139,  0.0769],\n",
       "         [ 0.5334,  0.6345, -0.3873,  ...,  0.4099,  0.6828,  0.4278],\n",
       "         [ 0.3844, -0.6489, -0.7329,  ..., -0.2930,  1.3756,  1.0907],\n",
       "         [-0.0317, -0.0615,  0.0414,  ..., -0.0523, -0.0358, -0.0208]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1019,  0.3926, -0.6049,  ...,  0.2273, -0.1167,  0.0931],\n",
       "         [ 0.8227,  0.4034, -0.9077,  ...,  0.6731,  1.0167,  0.4241],\n",
       "         [ 0.2165, -0.5795, -1.5805,  ..., -0.2217,  1.2788,  1.0567],\n",
       "         [-0.0663, -0.0983,  0.0123,  ..., -0.0384, -0.0046, -0.0681]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1960,  0.5765, -0.1576,  ...,  0.6488, -0.4976,  0.2176],\n",
       "         [ 0.3843, -0.0351, -1.3819,  ...,  1.1351,  0.8801,  0.4211],\n",
       "         [ 0.0299, -0.8636, -1.8141,  ...,  0.0912,  0.7426,  0.4424],\n",
       "         [-0.0744, -0.0368, -0.0035,  ..., -0.0019, -0.0233, -0.0700]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7784,  0.1870, -0.0939,  ...,  0.1641, -0.6622,  0.4254],\n",
       "         [ 0.2390,  0.3942, -0.8804,  ...,  0.7380,  0.6650,  0.6694],\n",
       "         [-0.2213, -1.4601, -1.3110,  ...,  0.7660,  0.6336,  0.1577],\n",
       "         [-0.0136, -0.0613,  0.0046,  ...,  0.0152, -0.0416, -0.0388]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.8660,  0.7642,  0.3275,  ...,  0.2510, -0.7047,  0.6182],\n",
       "         [-0.2676,  0.3260, -0.7583,  ...,  0.3147,  0.4308,  0.2149],\n",
       "         [-0.1061, -1.0634, -0.9522,  ...,  0.6768,  0.7062,  0.3375],\n",
       "         [-0.0054, -0.0729, -0.0088,  ..., -0.0231, -0.0228, -0.0687]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-5.2727e-01,  5.2474e-01,  5.8860e-01,  ...,  6.4532e-01,\n",
       "          -2.9931e-01,  5.8523e-01],\n",
       "         [-2.8599e-01, -1.6002e-01, -3.6921e-01,  ...,  2.8162e-01,\n",
       "           9.1919e-01,  3.5258e-01],\n",
       "         [-1.1821e-02, -7.5719e-01,  3.8038e-02,  ...,  8.4452e-01,\n",
       "           9.7246e-01,  2.8691e-01],\n",
       "         [ 2.3745e-04, -5.2459e-02,  8.9369e-03,  ..., -1.9477e-02,\n",
       "          -9.7219e-03, -5.4339e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2249,  0.3619,  0.2877,  ...,  0.3147,  0.0869, -0.4600],\n",
       "         [ 0.1082, -0.2197, -0.6319,  ...,  0.2270,  1.0346,  0.0849],\n",
       "         [ 0.6506, -0.5942, -0.0747,  ...,  0.2526,  1.0974, -0.0649],\n",
       "         [-0.0474, -0.0470,  0.0169,  ..., -0.0294,  0.0263, -0.0493]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2251,  0.3289,  0.3566,  ...,  0.1642,  0.2410, -0.6706],\n",
       "         [ 0.2576, -0.2807, -0.2994,  ..., -0.5026,  0.9895, -0.1119],\n",
       "         [ 0.7240, -0.5606,  0.2063,  ..., -0.2085,  0.9034, -0.3747],\n",
       "         [-0.0491, -0.0305,  0.0402,  ..., -0.0167,  0.0513, -0.0475]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2852,  0.4501,  0.2862,  ...,  0.4362,  0.4979, -0.2372],\n",
       "         [-0.2079,  0.0369,  0.0398,  ..., -0.5650,  0.3733,  0.0951],\n",
       "         [ 0.7796,  0.0815, -0.0798,  ..., -0.0146,  0.8973, -0.2835],\n",
       "         [-0.3637,  0.3618,  0.1234,  ...,  0.6489,  0.6392, -0.2132]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    embeddings = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2079,  0.0369,  0.0398,  ..., -0.5650,  0.3733,  0.0951],\n",
       "        [ 0.7796,  0.0815, -0.0798,  ..., -0.0146,  0.8973, -0.2835]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好 tensor([-2.0793e-01,  3.6918e-02,  3.9811e-02,  1.7311e-01,  1.3073e+00,\n",
      "        -1.1926e+00,  1.7733e-01,  3.7741e-01, -9.1014e-01,  1.3228e+00,\n",
      "         1.5831e-01, -3.7892e-01,  1.4100e+00, -3.5151e-01,  2.0441e+00,\n",
      "        -1.0238e+00,  7.2505e-01, -3.9941e-02, -7.0018e-01, -9.2059e-01,\n",
      "         8.3011e-01,  5.5756e-01, -1.8788e+00, -2.4900e-01,  8.9637e-01,\n",
      "         7.2073e-01, -7.6161e-01,  2.0468e-01,  9.4741e-01,  1.1170e+00,\n",
      "         2.6147e-01,  7.9134e-01, -1.3510e+00, -2.3269e-01,  6.5817e-01,\n",
      "        -5.2823e-02,  9.6547e-01,  6.0924e-01, -6.2540e-02, -6.5123e-01,\n",
      "        -6.3861e-01,  1.2747e+00,  2.5888e-01,  1.0379e-01,  2.3293e-01,\n",
      "         5.2488e-02,  2.8147e-01,  5.3963e-01, -1.1568e+00,  2.9894e-01,\n",
      "         6.4489e-01,  7.5942e+00,  3.2715e-01,  1.2338e+00,  2.5323e-01,\n",
      "         5.1191e-01,  2.9790e-01,  3.6909e-01, -4.6086e-02,  1.5273e-01,\n",
      "         2.2011e-02, -1.0319e+00,  9.1532e-01,  1.4048e+00,  3.3801e-01,\n",
      "        -1.0447e+00, -5.2274e-01, -1.6431e-01, -3.9692e-01, -1.1629e+00,\n",
      "         2.3886e-01,  1.1176e-01,  5.6126e-01,  5.3536e-01, -5.5928e-01,\n",
      "         6.2844e-01, -6.6733e-01,  1.5324e-01,  1.1143e-01, -4.7570e-01,\n",
      "         4.8987e-01,  4.3540e-01,  7.0049e-01, -3.0402e-01, -3.2770e-02,\n",
      "        -5.0026e-01, -3.9891e-01, -2.0501e+00,  5.0324e-01, -3.2111e-01,\n",
      "        -1.6345e-01, -7.8669e-01, -2.0361e-01, -2.4548e-01,  4.7807e-01,\n",
      "        -4.0231e-01,  2.4881e-01,  8.7605e-01, -6.4370e-02, -3.4295e-01,\n",
      "         1.3117e-01,  1.3917e+00, -3.0724e-01,  8.4813e-02, -1.5454e+00,\n",
      "        -4.4235e-01, -6.4208e-01, -7.6990e-01,  1.9916e-01, -8.1903e-01,\n",
      "        -9.6175e-01, -1.3269e-01,  2.6042e-01,  4.8917e-01, -3.3029e-01,\n",
      "        -3.9281e-02,  5.2325e-01,  1.6560e-01, -2.2799e+00, -3.7865e-01,\n",
      "         5.7062e-01, -3.6935e-02, -1.1923e+00, -5.9645e-01,  5.6253e-01,\n",
      "        -1.4779e+00, -3.7787e-02,  9.0090e-01,  3.9986e-01,  1.0141e-01,\n",
      "         8.1842e-01,  4.4620e-01, -1.3513e+00, -5.3640e-01,  1.5002e+00,\n",
      "         9.3141e-02, -4.1826e-01,  9.8447e-01, -3.0250e-01, -8.6032e-01,\n",
      "        -9.5837e-01, -3.7609e-01,  9.6949e-01,  1.5085e-01,  4.9873e-01,\n",
      "         1.1982e+00, -1.0577e+00,  4.3642e-01,  1.2775e+00,  1.5878e-01,\n",
      "        -5.0917e-01,  1.1035e+00, -8.2387e-01, -2.3350e-01, -4.0403e-02,\n",
      "        -1.2254e+00,  2.0282e-01, -4.9300e-01, -6.3078e-01,  3.9836e-01,\n",
      "         7.8564e-02,  4.9422e-01,  1.3356e-02, -9.5437e-01, -1.0890e+00,\n",
      "        -1.3344e-01,  1.0158e+00, -6.5010e-01,  2.1393e-01, -6.0092e-01,\n",
      "        -5.7037e-01,  3.9502e-01, -1.7973e-01,  8.3817e-01,  1.2769e+00,\n",
      "         3.4244e-01, -8.4229e-02, -5.6821e-01, -1.5543e+00,  4.0168e-01,\n",
      "        -1.1688e+00, -1.4909e-01,  5.4195e-01,  6.2340e-01, -1.3713e+00,\n",
      "        -1.9346e+00, -2.9529e-02,  9.5031e-01,  2.6541e-01,  2.0788e-01,\n",
      "        -2.6589e-01,  1.7603e-01, -1.7484e+00, -5.7530e-01,  6.4658e-01,\n",
      "         1.4247e+00, -1.2796e+00, -6.8229e-01, -5.7344e-01, -3.4741e-01,\n",
      "        -6.3047e-01, -1.0140e+00, -6.0393e-02, -4.3845e-01, -2.7503e-01,\n",
      "        -1.4473e-01, -7.3416e-01,  6.1396e-02,  9.0467e-01,  4.2026e-01,\n",
      "        -7.4666e-01, -1.7245e-01, -1.9816e-01,  3.4370e-02,  2.3624e-01,\n",
      "         3.5875e-01, -3.0740e-02, -5.2172e-01,  1.8523e-01, -2.1679e+00,\n",
      "         5.8596e-01,  3.6608e-01,  1.2507e+00, -1.4821e+00, -9.0034e-01,\n",
      "         1.9924e-02, -1.3501e-01,  3.3415e-01, -3.4137e-01,  4.1687e-01,\n",
      "         1.8393e-01,  3.7848e-01,  1.3855e-01, -6.1172e-04, -3.4679e-01,\n",
      "         6.1878e-01, -5.7938e-01,  1.5275e+00,  7.2378e-01, -1.8633e-01,\n",
      "         1.0275e+00,  2.4699e-01, -2.3180e-01, -4.7692e-01, -1.0950e+00,\n",
      "         3.0281e-02, -8.8040e-01,  1.3985e+00,  8.2579e-01,  5.5648e-01,\n",
      "         9.1270e-01,  5.1515e-01,  7.4496e-02, -1.4519e+00,  4.0595e-01,\n",
      "         1.5789e+00, -1.8036e-01, -4.6160e-01, -1.1350e+00, -2.4797e-01,\n",
      "         2.9997e-01, -8.8387e-02, -6.3229e-02,  7.9340e-01,  9.7193e-01,\n",
      "         3.4267e-01,  5.8844e-01, -1.1899e+00,  4.4356e-01, -1.9009e-01,\n",
      "         8.3135e-01,  9.7896e-02,  1.2600e-01, -1.0544e-01, -1.6808e+00,\n",
      "        -6.5688e-01,  1.5991e-01,  1.0149e+00,  1.2058e+00, -5.3453e-01,\n",
      "         2.1575e-01,  6.5736e-01,  4.0659e-01, -2.5589e-01, -1.1960e+00,\n",
      "         4.5816e-01,  1.2181e-01,  6.7467e-01,  4.8977e-01, -1.1081e+00,\n",
      "        -8.6951e-01,  3.6025e-01,  6.5920e-01, -2.6576e-01,  1.0364e+00,\n",
      "        -7.0341e-01,  6.4222e-01,  1.0343e+00, -2.4518e-01,  1.0896e+00,\n",
      "        -5.4394e-01, -4.0805e-01, -9.6964e-02,  6.0612e-02,  1.1300e-01,\n",
      "         4.3265e-01, -4.4012e-02,  9.9603e-01, -5.1125e-01, -6.7380e-01,\n",
      "         4.3839e-02, -1.3388e+00,  3.6869e-01,  2.9125e-01, -1.2511e+00,\n",
      "         1.7303e+00, -2.8280e-01, -7.3320e-01, -6.9049e-01,  1.5343e-01,\n",
      "         1.7381e-01,  2.7913e-01,  2.2437e-01,  1.1569e-01,  2.0616e-01,\n",
      "        -1.3251e-01, -5.9742e-02, -4.6000e-01,  4.6596e-01,  3.4880e-01,\n",
      "         3.6093e-01, -8.1948e-01, -5.6319e-02, -7.0204e-01, -3.2752e-01,\n",
      "         6.9096e-01, -8.1370e-01,  1.4290e+00, -6.1714e-01,  6.6342e-01,\n",
      "        -1.0993e-01,  1.8655e-01,  2.7448e-01, -1.7054e+00,  7.1562e-01,\n",
      "         5.5173e-01, -4.5198e-01, -8.6704e-01, -1.2438e+00,  9.8912e-01,\n",
      "        -1.5779e+00, -2.8118e-01,  7.7016e-01,  7.0500e-01, -1.6955e+00,\n",
      "         4.6586e-01,  2.7544e-01,  4.4170e-01,  9.5415e-01,  1.9051e-01,\n",
      "        -7.3781e-01, -6.8823e-02,  8.0615e-01, -1.3184e+00,  7.7625e-01,\n",
      "         1.3336e-01,  6.0008e-01,  2.5121e-01, -1.7430e-02, -8.2229e-01,\n",
      "         1.8930e-01,  2.7379e-02,  9.9676e-01, -8.9020e-01,  2.8682e-01,\n",
      "         7.1054e-01, -4.6986e-01, -5.8830e-01,  1.9849e+00, -2.3641e-01,\n",
      "        -1.6570e-01, -6.2092e-01, -4.1267e-01, -2.3006e+00, -1.4086e+00,\n",
      "         6.9036e-01, -1.2921e+00, -9.2864e-01,  2.9023e-01, -4.4242e-02,\n",
      "        -3.9880e-01,  1.5539e+00,  4.1218e-01, -4.0777e-01, -3.6445e-01,\n",
      "        -1.1449e+00, -9.9173e-02,  2.0961e-01, -3.3604e-01, -9.1785e-01,\n",
      "         9.3438e-01, -2.0195e-01,  2.8160e-01, -3.9346e-01, -1.3670e-01,\n",
      "        -3.3386e-01,  1.8025e-01, -5.8673e-01,  3.2496e-01,  6.0619e-01,\n",
      "        -8.6091e-01,  1.2069e+00,  1.1160e+00,  1.6730e-01, -6.7710e-01,\n",
      "        -3.7113e-01, -8.7369e-01, -1.3685e+00,  6.2448e-01, -7.7039e-01,\n",
      "        -2.2618e-01,  3.2748e-01, -7.8703e-01,  7.0598e-01,  3.7328e-01,\n",
      "        -8.9574e-01, -1.4805e-01, -7.3547e-01, -3.4452e-01, -7.4212e-01,\n",
      "        -1.2002e-01, -3.0310e-01,  3.5100e-02, -1.1040e-01,  3.8930e-01,\n",
      "         4.6035e-01, -8.6960e-01,  1.3427e-01, -7.0294e-01,  7.7320e-01,\n",
      "        -1.1463e+00, -1.0421e+00,  8.9540e-02,  1.5477e-02,  9.7990e-01,\n",
      "         5.1722e-01,  1.0582e+00, -1.7806e-01,  9.8528e-01, -1.5027e+00,\n",
      "        -4.1510e-01,  3.5913e-01,  9.9668e-02, -1.4016e+00,  6.5598e-01,\n",
      "        -8.0582e-01, -6.7137e-03, -6.6326e-01, -2.1564e-01,  2.1071e+00,\n",
      "         8.9142e-01,  4.6159e-01, -3.8912e-01,  1.6436e-01,  3.0788e-01,\n",
      "        -1.3122e-01, -3.0115e-01, -1.1541e+00,  9.0255e-01,  9.5851e-01,\n",
      "        -5.8679e-02,  6.3086e-01, -1.8771e-01,  6.9291e-01, -1.1083e+00,\n",
      "        -1.1078e+00, -4.8053e-01, -1.4879e+00, -7.7481e-01, -1.0493e+00,\n",
      "         3.2832e-01,  3.4497e-01,  1.2058e+00,  3.7086e-01,  1.2651e+00,\n",
      "        -3.9714e-01, -6.0182e-01, -2.8753e-02,  9.3444e-02, -2.8805e-01,\n",
      "        -4.5753e-01,  7.2712e-01,  2.3906e-01, -5.7972e-01, -2.7711e-01,\n",
      "        -1.1792e+00,  1.8407e-01,  1.0925e+00,  1.6554e-01,  1.5419e-01,\n",
      "        -6.1640e-01,  1.9429e-02,  8.3817e-01,  9.1780e-01, -6.4307e-01,\n",
      "        -6.8617e-01,  4.8297e-01, -5.1745e-01,  2.4320e-01,  6.1028e-01,\n",
      "         5.2404e-01,  3.1416e-01, -7.2308e-01,  1.4082e+00, -5.9859e-02,\n",
      "         3.0332e-01, -7.2512e-01, -4.4261e-02, -2.0483e-01,  1.0677e-01,\n",
      "        -1.7528e-01, -3.9817e-02, -1.9572e-02,  1.7046e-01, -4.4450e-01,\n",
      "        -5.3786e-01, -1.0788e+00,  1.5670e-01,  2.9822e-01,  1.5436e+00,\n",
      "         6.3133e-05, -7.4689e-01, -4.9409e-01, -5.0276e-01,  1.6322e+00,\n",
      "         5.6364e-01,  1.5541e+00, -1.4003e-01,  1.2960e+00,  4.9704e-01,\n",
      "        -1.5838e-01, -8.7368e-01,  1.3674e+00,  1.8352e+00, -4.0189e-02,\n",
      "        -1.7255e+00,  5.8941e-01,  3.6432e-02, -8.2124e-01,  5.9099e-01,\n",
      "         1.2454e-01,  1.8599e+00, -4.8773e-01,  1.1997e-01,  9.3780e-03,\n",
      "        -4.1725e-01, -1.7845e+00, -5.0450e-01,  7.9307e-01, -4.9869e-01,\n",
      "         2.1437e-02, -3.9648e-01,  1.9680e-01, -1.0723e+00, -1.2187e+00,\n",
      "         5.8921e-01,  1.1512e-03, -8.7917e-01,  5.5682e-02, -1.1511e+00,\n",
      "        -4.3998e-01,  7.9013e-01, -3.5374e-01,  3.4211e-01,  6.1265e-01,\n",
      "         1.1012e-01, -1.1563e+00, -8.5332e-01,  5.0136e-01,  1.1860e-01,\n",
      "        -4.8800e-01,  4.6713e-01, -7.1994e-01, -6.3333e-01,  4.4217e-01,\n",
      "        -1.0042e+00,  9.0339e-02,  9.5113e-02,  1.1154e+00, -2.0477e-01,\n",
      "         1.6541e+00, -1.7827e-01,  6.0061e-01,  2.4804e-02, -1.1195e+00,\n",
      "         5.4367e-01, -2.6039e-01, -4.2499e-01, -5.7927e-02, -3.1000e-02,\n",
      "        -9.5460e-01,  1.1000e+00,  1.9375e-02, -4.6645e-02,  9.0894e-01,\n",
      "        -5.4824e-02,  4.5281e-01,  7.0700e-01, -2.8325e-01,  8.9002e-01,\n",
      "        -1.1829e+00, -1.8395e-01,  1.5647e+00, -1.0873e-01, -2.1645e-02,\n",
      "         9.4743e-01, -1.0419e+00, -6.9803e-01,  2.5735e-01,  5.0419e-01,\n",
      "        -8.9934e-01, -7.2835e-01, -6.8351e-01, -6.1704e-03, -9.3094e-01,\n",
      "         2.0480e-01,  9.8301e-01, -4.3283e-01, -1.8034e-02,  3.4499e-01,\n",
      "        -2.2429e-01, -2.4470e-02,  2.6455e-01, -7.5637e-01, -1.0804e-02,\n",
      "         6.3850e-01,  3.9602e-02, -3.3892e-02,  2.6760e-01, -1.2341e+00,\n",
      "         7.0457e-01,  2.8168e-01, -5.9489e-01, -9.7336e-01, -6.3446e-01,\n",
      "        -7.2115e-01,  9.4800e-01, -1.2765e+00,  2.0843e+00, -1.2279e-01,\n",
      "         9.7045e-01,  1.0477e-01,  2.7661e-01, -2.1495e-01,  7.2834e-01,\n",
      "         1.7436e-01,  1.2191e+00, -7.1953e-01, -9.1530e-01, -1.6690e-01,\n",
      "        -2.5662e-02,  1.2377e-01,  1.0420e+00, -1.4126e-01,  6.1590e-01,\n",
      "        -8.9995e-01, -8.2605e-01,  3.0819e-01, -3.9621e-01, -2.3815e-03,\n",
      "        -3.0287e-02,  2.7309e-01,  1.0473e+00, -8.5713e-01, -4.1944e-02,\n",
      "         1.3207e+00,  9.3692e-01, -4.0087e-01, -5.0603e-01, -2.9836e-01,\n",
      "         4.6674e-02, -7.8422e-01,  3.0887e-01,  1.2751e-01, -4.3325e-01,\n",
      "         3.0823e-01,  3.7805e-01, -9.5003e-03, -5.1228e-01, -3.6242e-01,\n",
      "         3.6371e-01,  1.2903e+00, -3.0466e-01,  8.6092e-02, -1.0143e+00,\n",
      "        -1.6853e+00,  2.7704e-02,  1.1957e-01, -2.3084e-01,  1.0938e-01,\n",
      "         1.0803e+00,  8.5212e-01,  6.3117e-01, -4.9841e-01, -7.3853e-01,\n",
      "         1.0867e+00, -1.5027e+00,  4.8165e-01,  1.0630e+00,  6.9250e-01,\n",
      "        -8.2500e-01,  1.2635e-01,  7.2976e-01, -3.6123e-01, -6.4466e-02,\n",
      "        -5.7001e-01,  4.5262e-01, -1.1554e+00, -4.5896e-02,  2.9833e-01,\n",
      "         3.5697e-02,  8.3567e-02,  1.2932e+00, -4.0154e-01,  1.1187e+00,\n",
      "         2.4748e-01, -8.6170e-02,  9.3817e-02,  3.3210e-01,  1.3134e+00,\n",
      "        -9.7048e-01, -1.0916e+00,  1.3856e-01,  8.2446e-01, -1.4905e-01,\n",
      "         1.4133e+00,  9.1438e-01,  6.7851e-02,  1.2043e+00, -2.0645e-02,\n",
      "         3.0425e-01, -1.0571e+00, -2.0032e+00,  9.8541e-01, -6.5545e-01,\n",
      "        -7.5853e-01, -4.5234e-01, -5.3419e-01,  6.2619e-01,  8.4895e-01,\n",
      "         4.0845e-01,  6.7232e-01,  1.1243e+00, -2.2996e-01,  7.9307e-01,\n",
      "        -3.6980e-01,  4.7740e-01, -7.5320e-01,  2.1628e-02,  9.8720e-02,\n",
      "         3.9739e-01, -1.3949e+00,  2.9216e-01, -5.8255e-01, -1.6125e-01,\n",
      "        -5.6504e-01,  3.7335e-01,  9.5149e-02])\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(input_text.split()):\n",
    "    print(token, embeddings[0][i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pun_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
